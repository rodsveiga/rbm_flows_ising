{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rbm import RBM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with $ T = 1, 1.1, ..., 3.5$\n",
    "\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with $ T = 0$ and $ T = \\infty$\n",
    "\n",
    "In order to investigate the RBM flow in an extreme situation, we train the machine in an dataset composed only by frozen and random states. This still satisfies the condition shown in the work [Scale-invariant Feature Extraction of Neural Network and Renormalization Group Flow](https://arxiv.org/abs/1801.07172): the RBM flow would go towards the critical value if the data training set has states with temperature values less and greater than the critical one.\n",
    "\n",
    "It is crucial to note that, since the visible layer is always fed with an unidimensional vector (an array is transformed in a vector to be fed in the visible layer), the machine does not have any geometric information about the lattice in this case.\n",
    "\n",
    "### Case $T=0$ \"UP/DOWN\" and $ T = \\infty$\n",
    "\n",
    "#### Creating the dataset\n",
    "\n",
    "Frozen configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 1000\n",
    "\n",
    "training_lowT_up = nn.init.constant_(torch.empty(int(n_states/2), L*L),\n",
    "                                     val= 1.0)\n",
    "\n",
    "training_lowT_down = nn.init.constant_(torch.empty(int(n_states/2), L*L),\n",
    "                                     val= 0.0)\n",
    "\n",
    "training_set_ = torch.cat((training_lowT_down, training_lowT_up), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_highT = torch.empty(n_states, L*L).bernoulli_(p= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing and shuffling the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.cat((training_set_, training_highT), 0)\n",
    "\n",
    "training_set = training_set[torch.randperm(training_set.size()[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "For simplification, the units have no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nv = training_set.shape[1]\n",
    "Nh = training_set.shape[1]\n",
    "\n",
    "lr = 0.001\n",
    "k_learning = 1\n",
    "batch_size = 100\n",
    "nb_epoch = 448\n",
    "\n",
    "rbm = RBM(num_visible= training_set.shape[1], \n",
    "          num_hidden= Nh, \n",
    "          bias= False,\n",
    "          T= 1.0,\n",
    "          use_cuda= True)\n",
    "    \n",
    "rbm.learn(training_set= training_set,  \n",
    "          lr= lr, \n",
    "          nb_epoch= nb_epoch, \n",
    "          batch_size= batch_size,\n",
    "          k_learning= k_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RBM_model_zero_inf_T__UP_DOWN_nv%d_nh%d_lr%.1E_k%d_bsize%d_nepochs%d' % (Nv,\n",
    "                                                                                 Nh,\n",
    "                                                                                 lr,\n",
    "                                                                                 k_learning,\n",
    "                                                                                 batch_size,\n",
    "                                                                                 nb_epoch)\n",
    "\n",
    "PATH = 'RBM_trained_models/'+ name + '.pt'\n",
    "\n",
    "torch.save(rbm, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, v, h = rbm.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ = W.cpu().numpy().reshape((W.shape[0]*W.shape[1]))\n",
    "\n",
    "# Plot normalized histogram \n",
    "plt.hist(W_, bins= 100, density= True)\n",
    "\n",
    "# Maximum and minimum of xticks to compute the theoretical distribution \n",
    "x_min, x_max = min(plt.xticks()[0]), max(plt.xticks()[0])  \n",
    "domain = np.linspace(x_min, x_max, len(W_))\n",
    "\n",
    "# Fitting a normal distribution\n",
    "muW_, sigmaW_ = stats.norm.fit(W_) \n",
    "\n",
    "plot_pdf = stats.norm.pdf(domain, muW_, sigmaW_) # Fitting the PDF in the interval\n",
    "\n",
    "\n",
    "plt.plot(domain, plot_pdf, linewidth= 2.5,\n",
    "         label= '$\\mu= %f$ \\n$\\sigma^2$ = %f' % (muW_, sigmaW_**2 ))\n",
    "\n",
    "plt.title('Fitting a Normal Distribution for the weights ${\\cal W}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case $T=0$ \"UP\" and $ T = \\infty$\n",
    "\n",
    "#### Creating the dataset\n",
    "\n",
    "Frozen configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 1000\n",
    "\n",
    "training_lowT_up = nn.init.constant_(torch.empty(n_states, L*L),\n",
    "                                     val= 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_highT = torch.empty(n_states, L*L).bernoulli_(p= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing and shuffling the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.cat((training_lowT_up, training_highT), 0)\n",
    "\n",
    "training_set = training_set[torch.randperm(training_set.size()[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "For simplification, the units have no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nv = training_set.shape[1]\n",
    "Nh = training_set.shape[1]\n",
    "\n",
    "lr = 0.001\n",
    "k_learning = 1\n",
    "batch_size = 100\n",
    "nb_epoch = 305\n",
    "\n",
    "rbm = RBM(num_visible= training_set.shape[1], \n",
    "          num_hidden= Nh, \n",
    "          bias= False,\n",
    "          T= 1.0,\n",
    "          use_cuda= True)\n",
    "    \n",
    "rbm.learn(training_set= training_set,  \n",
    "          lr= lr, \n",
    "          nb_epoch= nb_epoch, \n",
    "          batch_size= batch_size,\n",
    "          k_learning= k_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RBM_model_zero_inf_T__UP_nv%d_nh%d_lr%.1E_k%d_bsize%d_nepochs%d' % (Nv,\n",
    "                                                                            Nh,\n",
    "                                                                            lr,\n",
    "                                                                            k_learning,\n",
    "                                                                            batch_size,\n",
    "                                                                            nb_epoch)\n",
    "\n",
    "PATH = 'RBM_trained_models/'+ name + '.pt'\n",
    "\n",
    "torch.save(rbm, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, v, h = rbm.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ = W.cpu().numpy().reshape((W.shape[0]*W.shape[1]))\n",
    "\n",
    "# Plot normalized histogram \n",
    "plt.hist(W_, bins= 100, density= True)\n",
    "\n",
    "# Maximum and minimum of xticks to compute the theoretical distribution \n",
    "x_min, x_max = min(plt.xticks()[0]), max(plt.xticks()[0])  \n",
    "domain = np.linspace(x_min, x_max, len(W_))\n",
    "\n",
    "# Fitting a normal distribution\n",
    "muW_, sigmaW_ = stats.norm.fit(W_) \n",
    "\n",
    "plot_pdf = stats.norm.pdf(domain, muW_, sigmaW_) # Fitting the PDF in the interval\n",
    "\n",
    "\n",
    "plt.plot(domain, plot_pdf, linewidth= 2.5,\n",
    "         label= '$\\mu= %f$ \\n$\\sigma^2$ = %f' % (muW_, sigmaW_**2 ))\n",
    "\n",
    "plt.title('Fitting a Normal Distribution for the weights ${\\cal W}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
